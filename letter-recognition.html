<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>LETTER RECOGNITION</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">usmanijaz </a></h1>
                <nav><ul>
                    <li><a href="/category/data-analysis.html">Data Analysis</a></li>
                    <li><a href="/category/linear-regression.html">Linear Regression</a></li>
                    <li><a href="/category/logistic-regression.html">Logistic Regression</a></li>
                    <li><a href="/category/text-analysis.html">Text Analysis</a></li>
                    <li class="active"><a href="/category/trees.html">Trees</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/letter-recognition.html" rel="bookmark"
           title="Permalink to LETTER RECOGNITION">LETTER RECOGNITION</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <span>Sat 20 June 2015</span>
        	<span>| by <a class="url fn" href="/author/usman-ijaz-malik.html">Usman Ijaz Malik</a></span>
	        <span>| in <a href="/category/trees.html">Trees</a></span>
<span>| tags: <a href="/tag/r.html">R</a><a href="/tag/data-analysis.html">Data Analysis</a></span>
</footer><!-- /.post-info -->      <p>One of the earliest applications of the predictive analytics methods was to automatically recognize letters, which post office machines use to sort mail.
In this problem, we will build a model that uses statistics of images of four letters in the Roman alphabet -- A, B, P, and R -- to predict which letter a 
particular image corresponds to.</p>
<p>This is a multiclass classification problem. We have mostly focused on binary classification problems (e.g., predicting whether an 
individual voted or not, whether or not a person is at risk for a certain disease, etc.). In this problem, we have more than two classifications
 that are possible for each observation. </p>
<p>The file <a href="https://dl.dropboxusercontent.com/u/95579925/Tree/letters_ABPR.csv">letters_ABPR.csv</a> contains 3116 observations, each of which corresponds to a certain image of one of the four letters A, B, P and R. 
The images came from 20 different fonts, which were then randomly distorted to produce the final images; each such distorted image is represented as a 
collection of pixels, each of which is "on" or "off". For each such distorted image, we have available certain statistics of the image in terms of these pixels,
 as well as which of the four letters the image is. This data comes from the UCI Machine Learning Repository.</p>
<p>This dataset contains the following 17 variables:</p>
<ul>
<li><strong>letter</strong> = the letter that the image corresponds to (A, B, P or R)</li>
<li><strong>xbox</strong> = the horizontal position of where the smallest box covering the letter shape begins.</li>
<li><strong>ybox</strong> = the vertical position of where the smallest box covering the letter shape begins.</li>
<li><strong>width</strong> = the width of this smallest box.</li>
<li><strong>height</strong> = the height of this smallest box.</li>
<li><strong>onpix</strong> = the total number of "on" pixels in the character image</li>
<li><strong>xbar</strong> = the mean horizontal position of all of the "on" pixels</li>
<li><strong>ybar</strong> = the mean vertical position of all of the "on" pixels</li>
<li><strong>x2bar</strong> = the mean squared horizontal position of all of the "on" pixels in the image</li>
<li><strong>y2bar</strong> = the mean squared vertical position of all of the "on" pixels in the image</li>
<li><strong>xybar</strong> = the mean of the product of the horizontal and vertical position of all of the "on" pixels in the image</li>
<li><strong>x2ybar</strong> = the mean of the product of the squared horizontal position and the vertical position of all of the "on" pixels</li>
<li><strong>xy2bar</strong> = the mean of the product of the horizontal position and the squared vertical position of all of the "on" pixels</li>
<li><strong>xedge</strong> = the mean number of edges (the number of times an "off" pixel is followed by an "on" pixel, or the image boundary is hit) as the image is scanned from left to right, along the whole vertical length of the image</li>
<li><strong>xedgeycor</strong> = the mean of the product of the number of horizontal edges at each vertical position and the vertical position</li>
<li><strong>yedge</strong> = the mean number of edges as the images is scanned from top to bottom, along the whole horizontal length of the image</li>
<li><strong>yedgexcor</strong> = the mean of the product of the number of vertical edges at each horizontal position and the horizontal position4</li>
</ul>
<p>Let's warm up by attempting to predict just whether a letter is B or not. To begin, let's load the file letters_ABPR.csv into R, and call it letters. 
Then, create a new variable isB in the dataframe, which takes the value "TRUE" if the observation corresponds to the letter B, and "FALSE" if it does not.
 We can do this by typing the following command into your R console:</p>
<div class="highlight"><pre><span></span><span class="x">letters = read.csv(&quot;letters_ABPR.csv&quot;)</span>
<span class="x">letters</span><span class="p">$</span><span class="nv">isB</span><span class="x"> = as.factor(letters</span><span class="p">$</span><span class="nv">letter</span><span class="x"> == &quot;B&quot;)</span>
</pre></div>


<p>Now let's split the data set into a training and testing set, putting 50% of the data in the training set. Set the seed to 1000 before making the split. 
The first argument to sample.split should be the dependent variable "letters$isB". Remember that TRUE values from sample.split should go in the training set.</p>
<div class="highlight"><pre><span></span><span class="x">library(caTools)</span>
<span class="x">set.seed(1000)</span>
<span class="x">spl = sample.split(letters</span><span class="p">$</span><span class="nv">isB</span><span class="x">, SplitRatio = 0.5)</span>
<span class="x">Train = subset(letters, spl==TRUE)</span>
<span class="x">Test = subset(letters, spl==FALSE)</span>
</pre></div>


<p>Before building models, let's consider a baseline method that always predicts the most frequent outcome, which is "not B". 
So, the accuracy of this baseline method on the test set is <strong>0.754172</strong></p>
<div class="highlight"><pre><span></span><span class="x">table(Test</span><span class="p">$</span><span class="nv">isB</span><span class="x">)</span>
<span class="x">1175/(1175+383)</span>
</pre></div>


<p>Now let's build a classification tree to predict whether a letter is a B or not, using the training set to build your model. 
Remember to remove the variable "letter" out of the model, as this is related to what we are trying to predict! 
To just remove one variable, we can either write out the other variables, or we can use the following notation:</p>
<div class="highlight"><pre><span></span>CARTb = rpart(isB ~ . - letter, data=train, method=&quot;class&quot;)
</pre></div>


<p>We are just using the default parameters in our CART model, so we don't need to add the minbucket or cp arguments at all. 
We also added the argument method="class" since this is a classification problem.</p>
<p>For the accuracy of the CART model on the test set  (Use type="class" when making predictions on the test set.)</p>
<div class="highlight"><pre><span></span><span class="x">predictCART = predict(CARTb,newdata = Test,type=&quot;class&quot;)</span>
<span class="x">table(Test</span><span class="p">$</span><span class="nv">isB</span><span class="x">,predictCART)</span>

<span class="x">(1113+340)/(1118+57+43+340)</span>
</pre></div>


<p>To compute the accuracy on the test set, we need to divide the sum of the true positives and true negatives 
by the total number of observations: (1118+340)/nrow(test) = <strong>0.9358151</strong></p>
<p>Now, let's build a random forest model to predict whether the letter is a B or not (the isB variable) using the training set. 
We should use all of the other variables as independent variables, except letter (since it helped us define what we are trying to predict!). 
Let's Use the default settings for ntree and nodesize (don't include these arguments at all). 
Right before building the model, let's set the seed to 1000. (NOTE: We might get a slightly different answer on this problem, 
even if we set the random seed. This has to do with our operating system and the implementation of the random forest algorithm.)</p>
<p>What is the accuracy of the model on the test set?</p>
<div class="highlight"><pre><span></span><span class="x">library(&quot;randomForest&quot;)</span>
<span class="x">set.seed(1000)</span>
<span class="x">ForestB = randomForest(isB ~ . - letter,data =Train)</span>
<span class="x">predictForest = predict(ForestB, newdata = Test)</span>
<span class="x">table(Test</span><span class="p">$</span><span class="nv">isB</span><span class="x">,predictForest)</span>
</pre></div>


<p>The accuracy of the model on the test set is the sum of the true positives and true negatives, divided by the total number of observations in the test set:</p>
<div class="highlight"><pre><span></span>(1165+374)/nrow(test) = **0.9878049**
</pre></div>


<p>Let us now move on to the problem that we were originally interested in, which is to predict whether or not a letter is one of the four letters A, B, P or R.</p>
<p>As we saw before that building a multiclass classification CART model in R is no harder than building the models for binary classification problems. 
Fortunately, building a random forest model is just as easy.</p>
<p>The variable in our data frame which we will be trying to predict is "letter". Let's start by converting letter in the original data set (letters) to a 
factor by running the following command in R:</p>
<div class="highlight"><pre><span></span><span class="x">letters</span><span class="p">$</span><span class="nv">letter</span><span class="x"> = as.factor( letters</span><span class="p">$</span><span class="nv">letter</span><span class="x"> )</span>
</pre></div>


<p>Now, let's generate new training and testing sets of the letters data frame using letters$letter as the first input to the sample.split function. 
Before splitting, set your seed to 2000. Again put 50% of the data in the training set. (Why do we need to split the data again? 
Remember that sample.split balances the outcome variable in the training and testing sets. With a new outcome variable, we want to re-generate our split.)</p>
<div class="highlight"><pre><span></span><span class="x">set.seed(2000)</span>
<span class="x">spl = sample.split(letters</span><span class="p">$</span><span class="nv">letter</span><span class="x">, SplitRatio = 0.5)</span>
<span class="x">train = subset(letters, spl==TRUE)</span>
<span class="x">test = subset(letters, spl==FALSE)</span>
</pre></div>


<p>In a multiclass classification problem, a simple baseline model is to predict the most frequent class of all of the options.</p>
<p>Then to compute the accuracy of the baseline method on the test set, we need to first figure out the most common outcome in the training set. 
The output of table(train$letter) tells us that "P" has the most observations. So we will predict P for all letters. 
On the test set, we can run the table command table(test$letter) to see that it has 401 observations that are actually P. 
So the test set accuracy of the baseline method is 401/nrow(test) = <strong>0.2573813</strong>.</p>
<p>Now let's build a classification tree to predict "letter", using the training set to build your model. We should use all of the other variables as independent 
variables, except "isB", since it is related to what we are trying to predict! We just need to use the default parameters in our CART model. 
Let's add the argument method="class" since this is a classification problem. 
Even though we have multiple classes here, nothing changes in how we build the model from the binary case.</p>
<div class="highlight"><pre><span></span>CARTabpr = rpart(letter ~ . - isB, data=train, method=&quot;class&quot;)
predictCARTabpr = predict(CARTabpr,newdata = test,type=&quot;class&quot;)
</pre></div>


<p>When we are computing the test set accuracy using the confusion matrix, we want to add everything on the main diagonal and divide by the total number of 
observations in the test set, which can be computed with nrow(test), where test is the name of your test set).</p>
<div class="highlight"><pre><span></span><span class="x">table(test</span><span class="p">$</span><span class="nv">letter</span><span class="x">,predictCARTabpr)</span>
<span class="x">(348+318+363+340)/nrow(test)</span>
</pre></div>


<p>So the accuracy is <strong> 0.8786906</strong>.</p>
<p>Now let's build a random forest model on the training data, using the same independent variables as in the previous problem -- 
again, don't forget to remove the isB variable. Let's just use the default parameter values for ntree and nodesize (we don't need to include these arguments at 
all). Let's set the seed to 1000 right before building your model. (Remember that we might get a slightly different result even if we set the random seed.)</p>
<div class="highlight"><pre><span></span>library(&quot;randomForest&quot;)
set.seed(1000)
Forestabpr = randomForest(letter ~ . - isB,data =train)
predictForestabpr = predict(Forestabpr, newdata = test)
</pre></div>


<p>And then we can compute the test set accuracy by looking at the confusion matrix table(test$letter, Forestabpr). 
The test set accuracy is the sum of the numbers on the main diagonal, divided by the total number of observations in the test set:</p>
<div class="highlight"><pre><span></span>(390+380 +393+364)/nrow(test) = 0.9801027
</pre></div>


<p>We found that the value is rather striking, for several reasons. The first is that it is significantly higher than the value for CART, 
highlighting the gain in accuracy that is possible from using random forest models. 
The second is that while the accuracy of CART decreased significantly as we transitioned from the problem of predicting B/not B 
(a relatively simple problem) to the problem of predicting the four letters (certainly a harder problem), the accuracy of the random forest model decreased by 
a tiny amount.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="https://www.linkedin.com/in/usmanijaz">LinkedIn</a></li>
                            <li><a href="">Email: uijaz59@gmail</a></li>
                            <li><a href="">Phone: +49 1573 8091889</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://www.facebook.com/usman.ijaz.77">Facebook</a></li>
                            <li><a href="https://twitter.com/uijaz59">Twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>