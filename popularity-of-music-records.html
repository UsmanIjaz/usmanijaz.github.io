<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>POPULARITY OF MUSIC RECORDS</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">usmanijaz </a></h1>
                <nav><ul>
                    <li><a href="/category/data-analysis.html">Data Analysis</a></li>
                    <li><a href="/category/linear-regression.html">Linear Regression</a></li>
                    <li class="active"><a href="/category/logistic-regression.html">Logistic Regression</a></li>
                    <li><a href="/category/text-analysis.html">Text Analysis</a></li>
                    <li><a href="/category/trees.html">Trees</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/popularity-of-music-records.html" rel="bookmark"
           title="Permalink to POPULARITY OF MUSIC RECORDS">POPULARITY OF MUSIC RECORDS</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <span>Tue 16 June 2015</span>
        	<span>| by <a class="url fn" href="/author/usman-ijaz-malik.html">Usman Ijaz Malik</a></span>
	        <span>| in <a href="/category/logistic-regression.html">Logistic Regression</a></span>
<span>| tags: <a href="/tag/r.html">R</a><a href="/tag/data-analysis.html">Data Analysis</a></span>
</footer><!-- /.post-info -->      <p>The music industry has a well-developed market with a global annual revenue around $15 billion. 
The recording industry is highly competitive and is dominated by three big production companies which make up nearly 82% of the total annual album sales. </p>
<p>Artists are at the core of the music industry and record labels provide them with the necessary resources to sell their music on a large scale.
A record label incurs numerous costs (studio recording, marketing, distribution, and touring) in exchange for a percentage of the profits from album sales, singles and concert tickets.</p>
<p>Unfortunately, the success of an artist's release is highly uncertain: a single may be extremely popular, resulting in widespread radio play and digital downloads,
 while another single may turn out quite unpopular, and therefore unprofitable. </p>
<p>Knowing the competitive nature of the recording industry, record labels face the fundamental decision problem of which musical releases to support to maximize their financial success. </p>
<p>How can we use analytics to predict the popularity of a song? We challenge ourselves to predict whether a song will reach a spot in the Top 10 of the Billboard Hot 100 Chart.</p>
<p>Taking an analytics approach, we aim to use information about a song's properties to predict its popularity. The dataset 
<a href="https://dl.dropboxusercontent.com/u/95579925/LoR/songs.csv">songs.csv</a> consists of all songs which made it to the Top 10 
of the Billboard Hot 100 Chart from 1990-2010 plus a sample of additional songs that didn't make the Top 10. 
This data comes from three sources: Wikipedia, Billboard.com, and EchoNest.</p>
<p>The variables included in the dataset either describe the artist or the song, or they are associated with the following song attributes:
 time signature, loudness, key, pitch, tempo, and timbre.</p>
<p>Here's a detailed description of the variables:</p>
<ul>
<li><strong>year</strong> = the year the song was released</li>
<li><strong>songtitle</strong> = the title of the song</li>
<li><strong>artistname</strong> = the name of the artist of the song</li>
<li><strong>songID and artistID</strong> = identifying variables for the song and artist</li>
<li><strong>timesignature and timesignature_confidence</strong> = a variable estimating the time signature of the song, and the confidence in the estimate</li>
<li><strong>loudness</strong> = a continuous variable indicating the average amplitude of the audio in decibels</li>
<li><strong>tempo and tempo_confidence</strong> = a variable indicating the estimated beats per minute of the song, and the confidence in the estimate</li>
<li><strong>key and key_confidence</strong> = a variable with twelve levels indicating the estimated key of the song (C, C#, . . ., B), and the confidence in the estimate</li>
<li><strong>energy</strong> = a variable that represents the overall acoustic energy of the song, using a mix of features such as loudness</li>
<li><strong>pitch</strong> = a continuous variable that indicates the pitch of the song</li>
<li><strong>timbre_0_min, timbre_0_max, timbre_1_min, timbre_1_max, . . . , timbre_11_min, and timbre_11_max</strong> = variables that indicate the minimum/maximum values over all segments for each of the twelve values in the timbre vector (resulting in 24 continuous variables)</li>
<li><strong>Top10</strong> = a binary variable indicating whether or not the song made it to the Top 10 of the Billboard Hot 100 Chart (1 if it was in the top 10, and 0 if it was not)</li>
</ul>
<p>Let's read the data:
    songs = read.csv("songs.csv")</p>
<p>Then, we can count the number of songs from 2010 by using the table function:</p>
<div class="highlight"><pre><span></span><span class="x">table(songs</span><span class="p">$</span><span class="nv">year</span><span class="x">)</span>
</pre></div>


<p>We have <strong>373</strong> observations from 2010.</p>
<p>Number of songs with the artist name Michael Jackson are <strong>18</strong>. We can find that out by typing:</p>
<div class="highlight"><pre><span></span>MichaelJackson = subset(songs, artistname == &quot;Michael Jackson&quot;)
nrow(MichaelJackson)
</pre></div>


<p>These are the songs by Michael Jackson which made to Top10. 
+   You Rock My World
+   You Are Not Alone
+   Black or White  <br />
+   Remember the Time
+   In The Closet   </p>
<p>We can find out this by typing the following code in R console.</p>
<div class="highlight"><pre><span></span><span class="x">subset(Michael</span><span class="p">$</span><span class="nv">songtitle</span><span class="x">,Michael</span><span class="p">$</span><span class="nv">Top10</span><span class="x">==1)</span>
</pre></div>


<p>The only values that appear in the table for timesignature are 0, 1, 3, 4, 5, and 7. 
We can also read from the table that 6787 songs have a value of 4 for the timesignature, which is the highest count out of all of the possible timesignature values.    </p>
<div class="highlight"><pre><span></span><span class="x">table(songs</span><span class="p">$</span><span class="nv">timesignature</span><span class="x">)</span>
</pre></div>


<p>The output of which.max(songs$tempo) is 6206, meaning that the song with the highest tempo is the row 6206. We can output the song title by typing:</p>
<div class="highlight"><pre><span></span><span class="x">songs</span><span class="p">$</span><span class="nv">songtitle</span><span class="x">[6206]</span>
</pre></div>


<p>The song title is: Wanna be Startin' Somethin'.</p>
<h1>Creating Our Prediction Model</h1>
<p>We wish to predict whether or not a song will make it to the Top 10. 
To do this, first we use the subset function to split the data into a training set "SongsTrain" consisting of all the observations up to and including 2009 song releases, 
and a testing set "SongsTest", consisting of the 2010 song releases.</p>
<div class="highlight"><pre><span></span><span class="x">SongsTrain = subset(songs,songs</span><span class="p">$</span><span class="nv">year</span><span class="x">&lt;=2009)</span>
<span class="x">SongsTest = subset(songs,songs</span><span class="p">$</span><span class="nv">year</span><span class="x">==2010)</span>
</pre></div>


<p>In this problem, our outcome variable is "Top10" - we are trying to predict whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart.
 Since the outcome variable is binary, we will build a logistic regression model. 
 We'll start by using all song attributes as our independent variables, which we'll call Model 1.</p>
<p>We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model. 
So we won't use the variables "year", "songtitle", "artistname", "songID" or "artistID".</p>
<p>We know that to build the logistic regression model, we would normally explicitly input the formula including all the independent variables in R. 
However, in this case, this is a tedious amount of work since we have a large number of independent variables.</p>
<p>There is a nice trick to avoid doing so. 
Let's suppose that, except for the outcome variable Top10, all other variables in the training set are inputs to Model1. Then, we can use the formula</p>
<div class="highlight"><pre><span></span>SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)
</pre></div>


<p>to build our model. Notice that the "." is used in place of enumerating all the independent variables.
 (Also, keep in mind that we can choose to put quotes around binomial, or leave out the quotes. R can understand this argument either way.)</p>
<p>However, in our case, we want to exclude some of the variables in our dataset from being used as independent variables ("year", "songtitle", "artistname", "songID", and "artistID"). 
To do this, we can use the following trick. 
First define a vector of variable names called nonvars - these are the variables that we won't use in our model.</p>
<div class="highlight"><pre><span></span>nonvars = c(&quot;year&quot;, &quot;songtitle&quot;, &quot;artistname&quot;, &quot;songID&quot;, &quot;artistID&quot;)
</pre></div>


<p>To remove these variables from our training and testing sets, type the following commands in our R console:</p>
<div class="highlight"><pre><span></span>SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]
</pre></div>


<p>Now, we can use the glm function to build a logistic regression model to predict Top10 using all of the other variables as the independent variables. 
We should use SongsTrain to build the model.</p>
<p>Looking at the summary of our model, the value of the Akaike Information Criterion (AIC) is <strong>4827.2</strong>.</p>
<div class="highlight"><pre><span></span>SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)
summary(SongsLog1)
</pre></div>


<p>Let's now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). 
Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). </p>
<p>If we look at the output summary(model), where model is the name of our logistic regression model, we can see that the coefficient estimates for the confidence variables 
(timesignature_confidence, key_confidence, and tempo_confidence) are positive. This means that higher confidence leads to a higher predicted probability of a Top 10 hit.</p>
<p>Since the coefficient values for timesignature_confidence, tempo_confidence, and key_confidence are all positive, lower confidence leads to a lower predicted probability 
of a song being a hit. So mainstream listeners tend to prefer less complex songs.</p>
<p>The coefficient estimate for loudness is positive, meaning that mainstream listeners prefer louder songs, which are those with heavier instrumentation. 
However, the coefficient estimate for energy is negative, meaning that mainstream listeners prefer songs that are less energetic, which are those with light instrumentation. 
These coefficients lead us to different conclusions!</p>
<p>The correlation between the variables "loudness" and "energy" in the training set is <strong>0.73991</strong> and can be computed by:</p>
<div class="highlight"><pre><span></span><span class="x">cor(SongsTrain</span><span class="p">$</span><span class="nv">loudness</span><span class="x">, SongsTrain</span><span class="p">$</span><span class="nv">energy</span><span class="x">)</span>
</pre></div>


<p>Given that these two variables are highly correlated, Model 1 suffers from multicollinearity. To avoid this issue, we will omit one of these two variables and rerun the logistic 
regression. In the rest of this problem, we'll build two variations of our original model: Model2, in which we keep "energy" and omit "loudness", and Model3, in which we keep 
"loudness" and omit "energy".</p>
<p>To create Model2, which is Model1 without the independent variable "loudness". This can be done with the following command:</p>
<div class="highlight"><pre><span></span>SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
</pre></div>


<p>We just subtracted the variable loudness. We couldn't do this with the variables "songtitle" and "artistname", because they are not numeric variables, and 
we might get different values in the test set that the training set has never seen. But this approach (subtracting the variable from the model formula) will always work when we 
want to remove numeric variables.</p>
<p>Looking at the summary of SongsLog2, and inspect the coefficient of the variable "energy", we can observe that Model2 suggests that songs with high energy levels tend to be more 
popular. This contradicts our observation in Model1.</p>
<p>Now, let's create Model3, which should be exactly like Model1, but without the variable "energy".</p>
<div class="highlight"><pre><span></span>SongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial)
</pre></div>


<p>Looking at the output of summary(SongsLog3), we can see that loudness has a positive coefficient estimate, meaning that our model predicts that songs with heavier
 instrumentation tend to be more popular. This is the same conclusion we got from Model2.</p>
<h1>Validating Our Model</h1>
<p>We can make predictions on the test set by using the command:</p>
<div class="highlight"><pre><span></span>testPredict = predict(SongsLog3, newdata=SongsTest, type=&quot;response&quot;)
</pre></div>


<p>Then, we can create a confusion matrix with a threshold of 0.45 by using the command:</p>
<div class="highlight"><pre><span></span><span class="x">table(SongsTest</span><span class="p">$</span><span class="nv">Top10</span><span class="x">, testPredict &gt;= 0.45)</span>
</pre></div>


<p>The accuracy of the model is (309+19)/(309+5+40+19) = <strong>0.87936</strong></p>
<p>Let's check if there's any incremental benefit in using Model3 instead of a baseline model. 
Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs.
We can compute the baseline accuracy by tabling the outcome variable in the test set:</p>
<div class="highlight"><pre><span></span><span class="x">table(SongsTest</span><span class="p">$</span><span class="nv">Top10</span><span class="x">)</span>
</pre></div>


<p>The baseline model would get 314 observations correct, and 59 wrong, for an accuracy of 314/(314+59) = <strong>0.8418231</strong>.  </p>
<p>It seems that Model3 gives us a small improvement over the baseline model. Still, does it create an edge?</p>
<p>Let's view the two models from an investment perspective. A production company is interested in investing in songs that are highly likely to make it to the Top 10. 
The company's objective is to minimize its risk of financial losses attributed to investing in songs that end up unpopular.</p>
<p>A competitive edge can therefore be achieved if we can provide the production company a list of songs that are highly likely to end up in the Top 10. 
We note that the baseline model does not prove useful, as it simply does not label any song as a hit. Let us see what our model has to offer.</p>
<p>Model 3 correctly predict <strong>19</strong> songs as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45.</p>
<p><strong>5</strong> non-hit songs our Model 3 predicts that will be Top 10 hits (again, looking at the test set), using a threshold of 0.45?</p>
<div class="highlight"><pre><span></span><span class="x">table(SongsTest</span><span class="p">$</span><span class="nv">Top10</span><span class="x">, testPredict &gt;= 0.45)</span>
</pre></div>


<p>We have 19 true positives (Top 10 hits that we predict correctly), and 5 false positives (songs that we predict will be Top 10 hits, but end up not being Top 10 hits).</p>
<p>We can compute the sensitivity to be 19/(19+40) = <strong>0.3220339</strong>, and the specificity to be 309/(309+5) = <strong>0.9840764</strong>.</p>
<p>Model 3 has a very high specificity, meaning that it favors specificity over sensitivity. 
While Model 3 only captures less than half of the Top 10 songs, it still can offer a competitive edge, since it is very conservative in its predictions.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="https://www.linkedin.com/in/usmanijaz">LinkedIn</a></li>
                            <li><a href="">Email: uijaz59@gmail</a></li>
                            <li><a href="">Phone: +49 1573 8091889</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://www.facebook.com/usman.ijaz.77">Facebook</a></li>
                            <li><a href="https://twitter.com/uijaz59">Twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>