<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>UNDERSTANDING WHY PEOPLE VOTE</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">usmanijaz </a></h1>
                <nav><ul>
                    <li><a href="/category/data-analysis.html">Data Analysis</a></li>
                    <li><a href="/category/linear-regression.html">Linear Regression</a></li>
                    <li><a href="/category/logistic-regression.html">Logistic Regression</a></li>
                    <li><a href="/category/text-analysis.html">Text Analysis</a></li>
                    <li class="active"><a href="/category/trees.html">Trees</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/understanding-why-people-vote.html" rel="bookmark"
           title="Permalink to UNDERSTANDING WHY PEOPLE VOTE">UNDERSTANDING WHY PEOPLE VOTE</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <span>Fri 19 June 2015</span>
        	<span>| by <a class="url fn" href="/author/usman-ijaz-malik.html">Usman Ijaz Malik</a></span>
	        <span>| in <a href="/category/trees.html">Trees</a></span>
<span>| tags: <a href="/tag/r.html">R</a><a href="/tag/data-analysis.html">Data Analysis</a></span>
</footer><!-- /.post-info -->      <p>In August 2006 three researchers (Alan Gerber and Donald Green of Yale University, and Christopher Larimer of the University of Northern Iowa) carried 
out a large scale field experiment in Michigan, USA to test the hypothesis that one of the reasons people vote is social, or extrinsic, pressure. 
To quote the first paragraph of their 2008 research paper:</p>
<p>"Among the most striking features of a democratic political system is the participation of millions of voters in elections. 
Why do large numbers of people vote, despite the fact that ... "the casting of a single vote is of no significance where there is a multitude of electors"?
 One hypothesis is adherence to social norms. Voting is widely regarded as a citizen duty, and citizens worry that others will think less of them if they fail 
 to participate in elections. Voters' sense of civic duty has long been a leading explanation of vote turnout..."</p>
<p>We will use both logistic regression and classification trees to analyze the data they collected.</p>
<p>The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a "control" group, 
and the rest were categorized into one of four "treatment" groups. These five groups correspond to five binary variables in the dataset.</p>
<ul>
<li><strong>"Civic Duty"</strong> (variable civicduty) group members were sent a letter that simply said "DO YOUR CIVIC DUTY - VOTE!"</li>
<li><strong>"Hawthorne Effect"</strong> (variable hawthorne) group members were sent a letter that had the "Civic Duty" message plus the additional message "YOU ARE BEING STUDIED" and they were informed that their voting behavior would be examined by means of public records.</li>
<li><strong>"Self"</strong> (variable self) group members received the "Civic Duty" message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records.</li>
<li><strong>"Neighbors"</strong> (variable neighbors) group members were given the same message as that for the "Self" group, except the message not only had the household voting records but also that of neighbors - maximizing social pressure.</li>
<li><strong>"Control"</strong> (variable control) group members were not sent anything, and represented the typical voting situation.</li>
<li>Additional variables include <strong>sex</strong> (0 for male, 1 for female), <strong>yob</strong> (year of birth), and the dependent variable <strong>voting</strong> (1 if they voted, 0 otherwise).</li>
</ul>
<p>We can load the dataset <a href="https://dl.dropboxusercontent.com/u/95579925/Tree/gerber.csv">gerber.csv</a> into R by using the read.csv command:</p>
<div class="highlight"><pre><span></span>gerber = read.csv(&quot;gerber.csv&quot;)
</pre></div>


<p>Then we can compute the percentage of people who voted by using the table function:</p>
<div class="highlight"><pre><span></span><span class="x">table(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">)</span>
</pre></div>


<p>The output tells us that 235,388 people did not vote, and 108,696 people did vote. This means that 108696/(108696+235388) = <strong>0.316</strong>
 of all people voted in the election.</p>
<p>In order to know which of the four "treatment groups" had the largest percentage of people who actually voted (voting = 1), we can type the following code in R.</p>
<div class="highlight"><pre><span></span><span class="x">tapply(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">, gerber</span><span class="p">$</span><span class="nv">civicduty</span><span class="x">, mean)</span>
<span class="x">tapply(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">, gerber</span><span class="p">$</span><span class="nv">hawthorne</span><span class="x">, mean)</span>
<span class="x">tapply(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">, gerber</span><span class="p">$</span><span class="nv">self</span><span class="x">, mean)</span>
<span class="x">tapply(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">, gerber</span><span class="p">$</span><span class="nv">neighbors</span><span class="x">, mean)</span>
</pre></div>


<p>The variable with the largest value in the "1" column has the largest fraction of people voting in their group - this is the Neighbors group.</p>
<p>We can build the logistic regression model with the following command:</p>
<div class="highlight"><pre><span></span>LogModel = glm(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, family=&quot;binomial&quot;)
</pre></div>


<p>If we look at the output of summary(LogModel), we can see that all of the variables are significant.</p>
<p>In order to calculate the accuracy of Logistic regression model with a threshold of greater then 0.3:</p>
<div class="highlight"><pre><span></span><span class="x">predictLog = predict(LogModel, type=&quot;response&quot;)</span>
<span class="x">table(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">, predictLog &gt; 0.3)</span>
</pre></div>


<p>We can compute the accuracy of the sum of the true positives and true negatives, divided by the sum of all numbers in the table:</p>
<div class="highlight"><pre><span></span>(134513+51966)/(134513+100875+56730+51966) = **0.542**
</pre></div>


<p>For a threshold of 0.5</p>
<div class="highlight"><pre><span></span><span class="x">table(gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">, predictLog &gt; 0.5)</span>
</pre></div>


<p>We can compute the accuracy of the sum of the true positives and true negatives, divided by the sum of all numbers in the table:</p>
<div class="highlight"><pre><span></span>(235388+0)/(235388+108696) = **0.684**
</pre></div>


<p>We can compute the AUC with the following commands (if your model's predictions are called "predictLog"):</p>
<div class="highlight"><pre><span></span><span class="x">library(ROCR)</span>
<span class="x">ROCRpred = prediction(predictLog, gerber</span><span class="p">$</span><span class="nv">voting</span><span class="x">)</span>
<span class="x">as.numeric(performance(ROCRpred, &quot;auc&quot;)@y.values)</span>
</pre></div>


<p>Even though all of our variables are significant, our model does not improve over the baseline model of just predicting that someone will not vote, 
and the <strong>AUC = 0.5308461</strong> is low. So while the treatment groups do make a difference, this is a weak predictive model.</p>
<h1>Trees</h1>
<p>We will now try out trees. We will build a CART tree for voting using all data and the same four treatment variables we used before.
We don't set the option method="class" - we are actually going to create a regression tree here. 
We are interested in building a tree to explore the fraction of people who vote, or the probability of voting. 
We’d like CART to split our groups if they have different probabilities of voting. If we used method=‘class’, 
CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% 
(since the predicted outcomes would be different). However, with regression trees, CART will split even if both groups have probability less than 50%.</p>
<div class="highlight"><pre><span></span>CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
</pre></div>


<p>If we plot the tree, with prp(CARTmodel), we should just see one leaf! There are no splits in the tree, because none of the variables make a big 
enough effect to be split on.</p>
<p>Now let's build the tree using the command:</p>
<div class="highlight"><pre><span></span>CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
</pre></div>


<p>to force the complete tree to be built. Then plot the tree. What do you observe about the order of the splits?</p>
<p>We can plot the tree with prp(CARTmodel2).</p>
<p><img alt="Photo" src="../images/RplotGerberTree.png" style="float: middle; width: 450px;"></p>
<p>As, we saw initially that the highest fraction of voters was in the Neighbors group, followed by the Self group, 
followed by the Hawthorne group, and lastly the Civic Duty group. And we see here that the tree detects this trend.</p>
<p>Using only the CART tree plot, we can determine what fraction (a number between 0 and 1) of "Civic Duty" people voted which is the value at the bottom right leaf <strong>0.31</strong></p>
<p>Let's make a new tree that includes the "sex" variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance 
to the treatment group.</p>
<div class="highlight"><pre><span></span>CARTmodel3 = rpart(voting ~ sex +civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel3)
</pre></div>


<p><img alt="Photo" src="../images/RplotGerberTree2.png" style="float: middle; width: 600px;"></p>
<p>We  can see that there is a split on the "sex" variable after every treatment variable split. 
For the control group, which corresponds to the bottom left, sex = 0 (male) corresponds to a higher voting percentage.</p>
<p>For the civic duty group, which corresponds to the bottom right, sex = 0 (male) corresponds to a higher voting percentage.</p>
<h1>Interaction Terms</h1>
<p>We know trees can handle "nonlinear" relationships, e.g. "in the 'Civic Duty' group and female", but it is possible to do the same for logistic regression. 
First, let's explore what trees can tell us some more.</p>
<p>Let's just focus on the "Control" treatment group. Let's Create a regression tree using just the "control" variable, 
then create another tree with the "control" and "sex" variables, both with cp=0.0.</p>
<div class="highlight"><pre><span></span>CARTcontrol = rpart(voting ~ control, data=gerber, cp=0.0)
CARTsex = rpart(voting ~ control + sex, data=gerber, cp=0.0)
</pre></div>


<p>Then, plot the "control" tree with the following command:</p>
<div class="highlight"><pre><span></span>prp(CARTcontrol, digits=6)
</pre></div>


<p><img alt="Photo" src="../images/RplotCartControl.png" style="float: middle; width: 600px;"> </p>
<p>The split says that if control = 1, predict 0.296638, and if control = 0, predict 0.34. The absolute difference between these is <strong>0.043362</strong>.</p>
<div class="highlight"><pre><span></span>prp(CARTsex, digits=6)
</pre></div>


<p><img alt="Photo" src="../images/RplotCartControlSex.png" style="float: middle; width: 600px;"><br />
The first split says that if control = 1, go left. Then, if sex = 1 (female) predict 0.290456, and if sex = 0 (male) predict 0.302795. 
On the other side of the tree, where control = 0, if sex = 1 (female) predict 0.334176, and if sex = 0 (male) predict 0.345818. So for 
women, not being in the control group increases the fraction voting by 0.04372. For men, not being in the control group increases the 
fraction voting by 0.04302. So men and women are affected about the same by NOT being in the control group (being in any of the four treatment groups).</p>
<p>Going back to logistic regression now,Let's create a model using "sex" and "control".</p>
<div class="highlight"><pre><span></span>LogModelSex = glm(voting ~ control + sex, data=gerber, family=&quot;binomial&quot;)
</pre></div>


<p>If we look at the summary of the model, we can see that the coefficient for the "sex" variable is -0.055791. 
This means that women are less likely to vote, since women have a larger value in the sex variable, and a negative coefficient means that 
larger values are predictive of 0.</p>
<p>The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), 
(Woman, Control). Logistic regression has attempted to do the same, although it wasn't able to do as well because it can't consider exactly the 
joint possibility of being a women and in the control group.</p>
<p>We can quantify this precisely. Let's create the following dataframe (this contains all of the possible values of sex and control), 
and evaluate our logistic regression using the predict function (where "LogModelSex" is the name of your logistic regression model that uses both control and sex):</p>
<div class="highlight"><pre><span></span>Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(LogModelSex, newdata=Possibilities, type=&quot;response&quot;)
</pre></div>


<p>The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), 
(Woman, Not Control), (Woman, Control) ). the absolute difference between the tree and the logistic regression for the (Woman, Control) case is:</p>
<p>The CART tree predicts 0.290456 for the (Woman, Control) case, and the logistic regression model predicts 0.2908065. 
So the absolute difference, to five decimal places, is <strong>0.00035</strong>.</p>
<p>So the difference is not too big for this dataset, but it is there. We're going to add a new term to our logistic regression now, 
that is the combination of the "sex" and "control" variables - so if this new variable is 1, that means the person is a woman AND in the control group. 
We can do that with the following command:</p>
<div class="highlight"><pre><span></span>LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family=&quot;binomial&quot;)
</pre></div>


<p>This coefficient is negative, so that means that a value of 1 in this variable decreases the chance of voting. 
This variable will have VALUE 1 if the person is a woman and in the control group.</p>
<p>Running the same code as before to calculate the average for each group:</p>
<div class="highlight"><pre><span></span>predict(LogModel2, newdata=Possibilities, type=&quot;response&quot;)
</pre></div>


<p>The logistic regression model now predicts 0.2904558 for the (Woman, Control) case, 
so there is now a very small difference (practically zero) between CART and logistic regression.    </p>
<p>This example has shown that trees can capture nonlinear relationships that logistic regression can not, but that we can get around this sometimes by 
using variables that are the combination of two variables.</p>
<p>But, We should not use all possible interaction terms in a logistic regression model due to overfitting. Even in this simple problem, we have four 
treatment groups and two values for sex. If we have an interaction term for every treatment variable with sex, we will double the number of variables. 
In smaller data sets, this could quickly lead to overfitting.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="https://www.linkedin.com/in/usmanijaz">LinkedIn</a></li>
                            <li><a href="">Email: uijaz59@gmail</a></li>
                            <li><a href="">Phone: +49 1573 8091889</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://www.facebook.com/usman.ijaz.77">Facebook</a></li>
                            <li><a href="https://twitter.com/uijaz59">Twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>